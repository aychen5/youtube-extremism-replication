zi_time_fs <- list(
zif_time_alternative_all,
zif_time_extremist_all,
zif_time_mainstream_all
)
ZIP_time_fit <- list()
for (i in 1:length(zi_time_fs)) {
ZIP_time_fit[[i]] <- pscl::zeroinfl(
zi_time_fs[[i]],
data = activity_data,
weights = weight_cmd,
offset = log(week),
dist = "poisson"
)
#print(summary(ZIP_time_fit[[i]]))
}
names(ZIP_time_fit) <-
c("time_alternative_full",
"time_extremist_full",
"time_mainstream_full")
time_plot_labels <- c(
"alternative" = str_wrap("Minutes/week on alternative channel videos", width = 30),
"extremist" = str_wrap("Minutes/week on extremist channel videos", width = 30),
"mainstream" = str_wrap("Minutes/week on mainstream media channel videos", width = 30)
)
robust_estimates_fxn <- function(model) {
result <- lmtest::coeftest(model, vcov = sandwich(model))
CIs <- confint(result) # honestly probably better to bootstrap, but w/e just be consistent
ci_lwr <- CIs[,1]
ci_upr <- CIs[,2]
stderrs <- result[,2]
pvals <- result[,4]
out <- tibble(ci_lwr,
ci_upr,
robust_se = stderrs,
pval = pvals,
estimate = result[,1],
covar = row.names(result)
) %>%
separate(covar,
into = c("component", "predictor"),
sep = "_",
extra = "merge")
return( out)
}
zip_coef_plot <- function(coef_df) {
if(unique(coef_df$component) %in% "zero") {
x_limit <-  c(-3.5, 3)
x_lab <- "Logit coefficient"
} else {
x_limit <- c(-5, 5)
x_lab <- "Poisson coefficient"
}
if( str_detect(deparse(substitute(coef_df)), "visit") ){
facet_lab <- visit_plot_labels
} else facet_lab <- time_plot_labels
coef_df %>%
ggplot(aes(x = estimate, y = str_wrap_factor(predictor, width = 12) )) +
geom_vline(xintercept = 0, lty = 2) +
geom_linerange(aes(xmin = ci_lwr, xmax = ci_upr,
color = channel_type),
lwd = 8,
show.legend = FALSE) +
geom_point(size = 10, shape = 21, stroke = 1,
color = "black", fill = "#FFFFFF") +
geom_text(aes(label = format(round(estimate, 2),nsmall = 2)) ,
size = 3.5, color = "black") +
geom_text(aes(label = stat_sig ),
size = 8, nudge_x = .5, nudge_y = .25) +
facet_wrap( ~ channel_type,
labeller = as_labeller(
facet_lab
)) +
scale_color_manual(values = c("alternative" = "#FFA500",
"extremist" = "#CD5C5C",
"mainstream" = "#015CB9")) +
labs(y = "",
x = x_lab
) +
scale_x_continuous(limits = x_limit) +
theme_bw() +
theme(strip.text = element_text(size = 12, hjust = .5),
strip.background = element_rect(fill = "grey",
color = "grey"),
axis.text.y = element_text(size = 12))
}
ZIP_time_df <- map_dfr(1:3, ~robust_estimates_fxn(ZIP_time_fit[[.x]])) %>%
mutate(channel_type = c(
rep("alternative", 20),
rep("extremist", 20),
rep("mainstream", 20)
),
stat_sig = case_when(estimate > 0 & ci_lwr > 0 ~ "*",
estimate < 0 & ci_upr < 0 ~ "*",
TRUE ~ ""))
ZIP_time_count_df <- ZIP_time_df %>%
filter(component == "count")  %>%
filter(predictor != "(Intercept)") %>%
mutate(predictor = refactor_fxn(recode_fxn(predictor)))
ZIP_time_zero_df <- ZIP_time_df %>%
filter(component == "zero") %>%
filter(predictor != "(Intercept)") %>%
mutate(predictor = refactor_fxn(recode_fxn(predictor)))
ZIP_time_count_plot <- zip_coef_plot(ZIP_time_count_df)
ZIP_time_zero_plot <- zip_coef_plot(ZIP_time_zero_df)
plot_grid(
ggplot(),
ZIP_time_zero_plot,
ggplot(),
ZIP_time_count_plot,
nrow = 4,
rel_heights = c(.1, 1, .1, 1),
vjust = 3,
labels = c("A: Zero component", "", "B: Count component", "")
)
ggsave("appendix-figures_files/zip_coefficient_time.pdf",
height = 14, width = 11 )
texreg(ZIP_time_fit,
digits = 2,
booktabs = TRUE,
label = "tab:figa5table",
caption = "Zero-inflated Poisson coefficients for correlates of the time per week spent on videos from alternative, extremist, and mainstream media channels. Robust standard errors are in parentheses.",
# texreg reads models as separate lists
override.pvalues = list(ZIP_time_df$pval[1:20],
ZIP_time_df$pval[21:40],
ZIP_time_df$pval[41:60]),
override.se = list(ZIP_time_df$robust_se[1:20],
ZIP_time_df$robust_se[21:40],
ZIP_time_df$robust_se[41:60]),
custom.model.names = c("Alternative",
"Extremist",
"Mainstream"),
custom.coef.map =
list(
"Count model: (Intercept)" = "Intercept",
"Count model: rr_cts" = "Racial resentment",
"Count model: jw_cts" = "Feeling Jews",
"Count model: fem_cts" = "Hostile sexism",
"Count model: age" = "Age",
"Count model: genderMale" = "Male",
"Count model: educ2Some college" = "Some college",
"Count model: educ24-year" = "Bachelor's degree",
"Count model: educ2Post-grad" = "Post-grad",
"Count model: raceNon-white" = "Non-white",
"Zero model: (Intercept)" = "Intercept",
"Zero model: rr_cts" = "Racial resentment",
"Zero model: jw_cts" = "Feeling Jews",
"Zero model: fem_cts" = "Hostile sexism",
"Zero model: age" = "Age",
"Zero model: genderMale" = "Male",
"Zero model: educ2Some college" = "Some college",
"Zero model: educ24-year" = "Bachelor's degree",
"Zero model: educ2Post-grad" = "Post-grad",
"Zero model: raceNon-white" = "Non-white"
),
reorder.coef = c(seq(1,20, by=2),
seq(2, 20, by=2)),
file = "appendix-figures_files/figureA5.tex")
# DV is view counts
f_visit_alternative_all <- formula(activity_yt_n_video_alternative_all ~  rr_cts + jw_cts + fem_cts + age + gender + educ2 + race )
f_visit_extremist_all <- formula(activity_yt_n_video_extremist_all ~  rr_cts + jw_cts + fem_cts + age + gender + educ2 + race)
f_visit_mainstream_all <- formula(activity_yt_n_video_mainstream_all  ~  rr_cts + jw_cts + fem_cts + age + gender + educ2 + race )
main_visit_fs <- list(
f_visit_alternative_all,
f_visit_extremist_all,
f_visit_mainstream_all
)
QP_visit_fit <- list()
for (i in 1:length(main_visit_fs)) {
QP_visit_fit[[i]] <-
robust_weighted_quasipoisson(
formula = main_visit_fs[[i]]
)
#print(QP_visit_fit[[i]])
}
names(QP_visit_fit) <-
c(
"visit_alternative_full",
"visit_extremist_full",
"visit_mainstream_full"
)
coef_names <- c("Intercept",
"Racial resentment",
"Feeling Jews",
"Hostile sexism",
"Age",
"Male",
"Some college",
"Bachelor's degree",
"Post-grad",
"Non-white")
visit_models <- bind_rows(QP_visit_fit)
visit_plot_labels <- c(
"alternative" = str_wrap("Views/week to alternative channel videos", width = 30),
"extremist" = str_wrap("Views/week to extremist channel videos", width = 30),
"mainstream" = str_wrap("Views/week to mainstream media channel videos", width = 30)
)
visit_models %>%
filter(predictor != "(Intercept)") %>%
mutate(predictor = refactor_fxn(recode_fxn(predictor))) %>%
ggplot(aes(x = estimate, y = str_wrap_factor(predictor, width = 12) )) +
geom_vline(xintercept = 0, lty = 2) +
geom_linerange(aes(xmin = ci_lwr, xmax = ci_upr,
color = channel_type),
size = 9,
show.legend = FALSE) +
geom_point(size = 10, shape = 21, stroke = 1,
color = "black", fill = "#FFFFFF") +
geom_text(aes(label = round(estimate, 2)),
size = 3.5, color = "black") +
geom_text(aes(label = stat_sig ),
size = 8, nudge_x = .45, nudge_y = .25) +
facet_wrap( ~ channel_type,
labeller = as_labeller(
visit_plot_labels
)) +
scale_color_manual(values = c("alternative" = "#FFA500",
"extremist" = "#CD5C5C",
"mainstream" = "#015CB9")) +
labs(y = "", x = "Quasipoisson coefficient") +
theme_bw() +
theme(strip.text = element_text(size = 12, hjust = .5),
strip.background = element_rect(fill="grey", color = "grey"),
axis.text.y = element_text(size = 12))
ggsave("appendix-figures_files/qpois_coefficient_visit.pdf",
height = 6, width = 11 )
QP_visit_fit_nonrob <- list()
for (i in 1:length(main_visit_fs)) {
QP_visit_fit_nonrob[[i]] <-
robust_weighted_quasipoisson(formula = main_visit_fs[[i]],
robust_output = F)
}
robust_SEs <- visit_models$robust_se
robust_pvals<- visit_models$p_val
texreg(QP_visit_fit_nonrob,
digits = 2,
booktabs = TRUE,
label = "tab:figa6table",
caption = "Quasipoisson coefficients for correlates of time per week spent on videos from alternative, extremist, and mainstream media channels. Robust standard errors are in parentheses.",
# texreg reads models as separate lists
override.pvalues = list(robust_pvals[1:10],
robust_pvals[11:20],
robust_pvals[21:30]),
override.se = list(robust_SEs[1:10],
robust_SEs[11:20],
robust_SEs[21:30]),
custom.model.names = c("Alternative",
"Extremist",
"Mainstream"),
custom.coef.map =
list(
"fem_cts" = "Hostile sexism",
"rr_cts" = "Racial resentment",
"jw_cts" = "Feeling Jews",
"age" = "Age",
"genderMale" =  "Male",
"raceNon-white" = "Non-white",
"educ2Some college" = "Some college",
"educ24-year" = "Bachelor's degree",
"educ2Post-grad" = "Post-grad",
"(Intercept)" = "Intercept"
),
file = "appendix-figures_files/figureA6.tex")
### --------- specify a bunch of variables we want to look at
channel_types <- c("alternative",
"extremist",
"mainstream",
"other")
# proportion that watched any alt/ext/msm videos
bh_dummy <- c("bh_alternative_any",
"bh_extremist_any",
"bh_mainstream_any",
"bh_other_any")
at_dummy <- c("at_alternative_any",
"at_extremist_any",
"at_mainstream_any",
"at_other_any")
bh_counts <- paste0("browser_history_yt_n_video_",
channel_types,
"_all")
bh_time <-
paste0("browser_history_yt_video_time_elapsed_capped_total_",
channel_types,
"_all")
at_counts <- paste0("activity_yt_n_video_",
channel_types,
"_all")
at_time <- paste0("activity_yt_video_time_elapsed_capped_total_",
channel_types,
"_all")
# number subscribed to alternative
bh_subscribed_counts <- paste0("browser_history_yt_n_video_",
channel_types,
"_subscribed")
at_subscribed_counts  <-  paste0("activity_yt_n_video_",
channel_types,
"_subscribed")
# time on subscribed to alternative
bh_subscribed_time <- paste0("browser_history_yt_video_time_elapsed_capped_total_",
channel_types,
"_subscribed")
at_subscribed_time  <-  paste0("activity_yt_video_time_elapsed_capped_total_",
channel_types,
"_subscribed")
# put activity and browser history together
select_activity_data_data <- activity_data %>%
select(caseid, at_alt, at_ext, at_msm, at_other, activity_n_total,
all_of(at_counts),
all_of(at_time),
all_of(at_subscribed_counts),
all_of(at_subscribed_time))
select_browser_history_data <- browser_history_data %>%
select(caseid, bh_alt, bh_ext, bh_msm, bh_other, browser_history_n_total,
all_of(bh_counts),
all_of(bh_time),
all_of(bh_subscribed_counts),
all_of(bh_subscribed_time))
merged_data <-
full_join(select_activity_data_data, select_browser_history_data,
by = "caseid") %>%
rename( bh_alternative_any = bh_alt,
bh_extremist_any = bh_ext,
bh_mainstream_any = bh_msm,
bh_other_any = bh_other,
at_alternative_any = at_alt,
at_extremist_any = at_ext,
at_mainstream_any = at_msm,
at_other_any = at_other)
correlation_fxn <- function(bh_var, at_var, data = merged_data) {
r = cor(x = data[[bh_var]],
y = data[[at_var]],
use = "complete.obs", method = "pearson")
out <- tibble(corr = r,
channel_type = str_extract(at_var,
paste0(channel_types, collapse = "|") ),
metric = case_when(
# youtube general
str_detect(at_var, "_yt_n_total") ~ "total yt",
# youtube channel type
str_detect(at_var, "any") ~ "binary",
str_detect(at_var, "subscribed") &
str_detect(at_var, "yt_n_video") ~ "sub count",
str_detect(at_var, "subscribed") &
str_detect(at_var, "time") ~ "sub time",
str_detect(at_var, "yt_n_video") ~ "count",
str_detect(at_var, "time") ~ "time"
))
return(out)
}
correlation_plot <- function(bh_var, at_var, data = merged_data) {
r = cor(x = data[[bh_var]],
y = data[[at_var]],
use = "complete.obs", method = "pearson")
out <- ggplot(aes(x = data[[bh_var]],
y = data[[at_var]]),
data = data) +
geom_point() +
geom_abline()
return(out)
}
# any visits to channel type
corr_binary <- map2(.x = bh_dummy, .y = at_dummy,
correlation_fxn) %>%
bind_rows()
# number of videos
corr_counts <- map2(.x = bh_counts, .y = at_counts,
correlation_fxn) %>%
bind_rows()
# capped time on videos
corr_time <- map2(.x = bh_time, .y = at_time,
correlation_fxn) %>%
bind_rows()
# number channel visit
# corr_channel <- map2(.x = bh_subscribed_counts, .y = at_subscribed_counts,
#      correlation_fxn) %>%
#   bind_rows()
# number subscribed
corr_subs_count <- map2(.x = bh_subscribed_counts, .y = at_subscribed_counts,
correlation_fxn) %>%
bind_rows()
# time subscribed
corr_subs_time <- map2(.x = bh_subscribed_time, .y = at_subscribed_time,
correlation_fxn) %>%
bind_rows()
# youtube correlations
youtube_corr_df <- corr_binary %>%
bind_rows(corr_counts) %>%
bind_rows(corr_time) %>%
bind_rows(corr_subs_count) %>%
bind_rows(corr_subs_time) %>%
mutate(channel_type = recode_channel_type_fxn(channel_type),
metric = factor(metric, levels = c(
"binary",
"count", "sub count",
"time", "sub time"
)))
youtube_channel_type <- youtube_corr_df %>%
ggplot(aes(x = channel_type,
y = 1,
fill = corr)) +
geom_tile(color = "black") +
geom_text(aes(label = format(round(corr, 2), nsmall = 2)),
color = "white", size = 6) +
scale_fill_gradient2(low = "#FFFFFF",
mid = "#6F9EBE",
high = "#032146",
midpoint = .5,
limits = c(0, 1),
breaks = seq(0, 1, .2)) +
coord_fixed(ratio = 1/2) +
facet_wrap(~metric, nrow = 5,
labeller = labeller(
metric = c("binary" = "Any views of YouTube videos from channel",
"count" = "Total number of views of YouTube videos from channel",
"time" = "Seconds elapsed on YouTube videos from channel (capped)",
"sub count" = "Total number of views of subscribed channel",
"sub time" = "Seconds elapsed on videos from subscribed channel (capped)")
)) +
labs(x = "", y = "") +
theme(axis.text.y = element_blank(),
panel.grid = element_blank(),
strip.text.x = element_text(size = 12, hjust = 0))+
guides(fill = guide_colourbar(barwidth = 20,
barheight = 1,
title = ""))
youtube_channel_type
ggsave("appendix-figures_files/correlations_history_activity_youtube_channel_type.pdf",
height = 8, width = 9 )
youtube_channel_type <- youtube_corr_df %>%
ggplot(aes(x = channel_type,
y = 1,
fill = corr)) +
geom_tile(color = "black") +
geom_text(aes(label = format(round(corr, 2), nsmall = 2)),
color = "white", size = 6) +
scale_fill_gradient2(low = "#FFFFFF",
mid = "#6F9EBE",
high = "#032146",
midpoint = .5,
limits = c(0, 1),
breaks = seq(0, 1, .2)) +
coord_fixed(ratio = 1/2) +
facet_wrap(~metric, nrow = 5,
labeller = labeller(
metric = c("binary" = "Any views of YouTube videos from channel",
"count" = "Total number of views of YouTube videos from channel",
"time" = "Seconds elapsed on YouTube videos from channel (capped)",
"sub count" = "Total number of views of subscribed channel",
"sub time" = "Seconds elapsed on videos from subscribed channel (capped)")
)) +
labs(x = "", y = "") +
theme(axis.text.y = element_blank(),
panel.grid = element_blank(),
legend.position = 'bottom',
strip.text.x = element_text(size = 12, hjust = 0))+
guides(fill = guide_colourbar(barwidth = 20,
barheight = 1,
title = ""))
youtube_channel_type
ggsave("appendix-figures_files/correlations_history_activity_youtube_channel_type.pdf",
height = 8, width = 9 )
### ====== survey measures for those who did and did not install the extension
yg_youtube <-  merged_data %>%
select(user_id, weight_cmd, browser_sample,
starts_with("youtube"),
-youtube_vid_quality,
-youtube_freq_use_3,
-youtube_freq_use)
merged_data
bh_at_data <-
full_join(select_activity_data_data, select_browser_history_data,
by = "caseid") %>%
rename( bh_alternative_any = bh_alt,
bh_extremist_any = bh_ext,
bh_mainstream_any = bh_msm,
bh_other_any = bh_other,
at_alternative_any = at_alt,
at_extremist_any = at_ext,
at_mainstream_any = at_msm,
at_other_any = at_other)
correlation_fxn <- function(bh_var, at_var, data = bh_at_data) {
r = cor(x = data[[bh_var]],
y = data[[at_var]],
use = "complete.obs", method = "pearson")
out <- tibble(corr = r,
channel_type = str_extract(at_var,
paste0(channel_types, collapse = "|") ),
metric = case_when(
# youtube general
str_detect(at_var, "_yt_n_total") ~ "total yt",
# youtube channel type
str_detect(at_var, "any") ~ "binary",
str_detect(at_var, "subscribed") &
str_detect(at_var, "yt_n_video") ~ "sub count",
str_detect(at_var, "subscribed") &
str_detect(at_var, "time") ~ "sub time",
str_detect(at_var, "yt_n_video") ~ "count",
str_detect(at_var, "time") ~ "time"
))
return(out)
}
correlation_plot <- function(bh_var, at_var, data = merged_data) {
r = cor(x = data[[bh_var]],
y = data[[at_var]],
use = "complete.obs", method = "pearson")
out <- ggplot(aes(x = data[[bh_var]],
y = data[[at_var]]),
data = data) +
geom_point() +
geom_abline()
return(out)
}
